{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization using TextRank Algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1zacjKj3zR2OroL_PZdW-vFvFAbXdjcBD",
      "authorship_tag": "ABX9TyM06Ix1Fn6g1fiPsMFME99i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjith13119/Seq2Seq---Text-Summarization/blob/main/Text_Summarization_using_TextRank_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oneScUlS8kg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSfnsrZGc-V7",
        "outputId": "a2fd61d1-f1cd-4acc-9673-2790b364f7fb"
      },
      "source": [
        "# import os\n",
        "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/Kaggle\"\n",
        "# %cd /content/drive/MyDrive/NLP/unsupervised Text summary\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP/unsupervised Text summary\n",
            "--2021-06-25 13:56:08--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-06-25 13:56:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-06-25 13:56:09--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    in 2m 41s  \n",
            "\n",
            "2021-06-25 13:58:51 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKWWjRyJcl3g"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/NLP/unsupervised Text summary/tennis_articles.csv\", encoding= 'unicode_escape')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNXILsc-f5Nu",
        "outputId": "7260c5f3-c29a-4442-d733-c0e798bcd2c2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence = []\n",
        "\n",
        "for sent in data.article_text:\n",
        "  sentence.append(sent_tokenize(sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP1tPhwfgM3W"
      },
      "source": [
        "sentence = [y for x in sentence for y in x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm6JJoINgiqI"
      },
      "source": [
        "word_embedding = {}\n",
        "f = open(\"/content/drive/MyDrive/NLP/unsupervised Text summary/glove.6B.100d.txt\", encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlhHT-pfguRW"
      },
      "source": [
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  word_embedding[word]  = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QOrJ10YiP9l",
        "outputId": "83fe868b-1eb4-4bd6-b689-6df7b8036117"
      },
      "source": [
        "len(word_embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiNBd8H9ij4M"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "\n",
        "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKWiUZqii2ch",
        "outputId": "6955c4d7-7e05-4c48-bf89-47fc8352535d"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')\n",
        "import re     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO6dev5MiR_U"
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "clean_sentences = pd.Series(sentence).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "# make alphabets lowercase\n",
        "clean_sentences = [s.lower() for s in clean_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThsbsGGWsmMd"
      },
      "source": [
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6RWkiV2jBxM"
      },
      "source": [
        "cleaned_texts = []\n",
        "for t in sentence:\n",
        "  cleaned_texts.append(text_preprocessing(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8M_Rlxsjb4-"
      },
      "source": [
        "cleaned_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK1hT4-ijhKD"
      },
      "source": [
        "sentence_vectors = []\n",
        "for i in cleaned_texts:\n",
        "  if len(i) != 0:\n",
        "    # first fetch vectors (each of size 100 elements) for the constituent words in a sentence \n",
        "    # and then take mean/average of those vectors to arrive at a consolidated vector for the sentence\n",
        "    v = sum([word_embedding.get(w,100) for w in i])/ (len(i) + 0.001) \n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "\n",
        "  sentence_vectors.append(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib61rIJonkxN"
      },
      "source": [
        "sim_mat = np.zeros([len(sentence), len(sentence)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzNHBgB7nk2t"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3xCQZOYnk5U"
      },
      "source": [
        "for i in range(len(sentence)):\n",
        "  for k in range(len(sentence)):\n",
        "    if i != k:\n",
        "      sim_mat[i][k] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[k].reshape(1,100))[0][0] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTP2a-Z6oo1e"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph, max_iter=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdhuWyUzrxQL"
      },
      "source": [
        "Finally, it’s time to extract the top N sentences based on their rankings for summary generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx3tHPY2rvOE"
      },
      "source": [
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentence)), reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoaBallhs5Gl",
        "outputId": "ab8f9fcc-01a4-4292-9f79-62f26ba5b62a"
      },
      "source": [
        "for i in range(10):\n",
        "  print(ranked_sentences[i][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fourteen years after sharing centre court with Roger Federer in the third round of the Australian Open, Reid was now running the counter and coaching kids - but sadly now never mine- in suburban Sydney at the Matraville tennis and squash centre run by his father Bob and beautiful mother Sandra, herselfa former professional.\n",
            "But my fondest memory is probably him coming back in Spain with his father and Todd trying to order a ham, cheese and tomato sandwich in the most Aussie English I ever heard. Among his lesser known achievements - but even more significantto me - was an injured Reids effort to reach the final of the Manly Seaside, a prestigious Christmas-time event won backin the day by the likes of legends Rod Laver, Ken Rosewall, Frank Sedgman, John Bromwich, Lew Hoad, Neal Fraser, Fred Stolle, Evonne Goolagong and Australias last womens Australian Open champion, Christine ONeil.\n",
            "Hed backed up his last-32 showingat Melbourne Park with a string of wins over elites including French Open champion and then world No.9 Gaston Gaudio and Roland Garros runner-up Martin Verkerk in 2004 before illness struck.\n",
            "The 20-time Grand Slam winner is chasing his 99th ATP title at the Swiss Indoors this week and he faces Jan-Lennard Struff in the second round on Thursday (6pm BST).\n",
            "I was on a nice trajectorythen, Reid recalled.If I hadnt got sick, I think I could have started pushing towards the second week at the slams and then who knows. Duringa comeback attempt some five years later, Reid added Bernard Tomic and 2018 US Open Federer slayer John Millman to his list of career scalps.\n",
            "Two players, Stefanos Tsitsipas and Kyle Edmund, won their first career ATP titles last week (13:26).\n",
            "British No 1 Kyle Edmund is the 12th seed in Paris and will get underway in round two against either Karen Khachanov or Filip Krajinovic.\n",
            "I didn't serve very well [against first-round opponent Filip Kranjovic, Federer said.\n",
            "Djokovic could play Marco Cecchinato in the second round.\n",
            "A big fan and believer in the enigmatic Canberran, Reid didnt want to see Kyrgioss career slip away like his did.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2fQcSWG-Dpz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raH8NMAg-DsE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJkAcPQl-DvF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from scipy import spatial\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s84oaBlg-WfQ",
        "outputId": "5fb5e3fa-0192-4f59-b839-215473f3b088"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I3EV0VI-FeW"
      },
      "source": [
        "text='''Santiago is a Shepherd who has a recurring dream which is supposedly prophetic. Inspired on learning this, he undertakes a journey to Egypt to discover the meaning of life and fulfill his destiny. During the course of his travels, he learns of his true purpose and meets many characters, including an “Alchemist”, that teach him valuable lessons about achieving his dreams. Santiago sets his sights on obtaining a certain kind of “treasure” for which he travels to Egypt. The key message is, “when you want something, all the universe conspires in helping you to achieve it.” Towards the final arc, Santiago gets robbed by bandits who end up revealing that the “treasure” he was looking for is buried in the place where his journey began. The end.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99RWY_LT-JFS"
      },
      "source": [
        "sentence_tokens = sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIWwH5fv-YWx",
        "outputId": "ef6bbec9-867a-4d10-f0a3-4b6fa069c028"
      },
      "source": [
        "sentence_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Santiago is a Shepherd who has a recurring dream which is supposedly prophetic.',\n",
              " 'Inspired on learning this, he undertakes a journey to Egypt to discover the meaning of life and fulfill his destiny.',\n",
              " 'During the course of his travels, he learns of his true purpose and meets many characters, including an “Alchemist”, that teach him valuable lessons about achieving his dreams.',\n",
              " 'Santiago sets his sights on obtaining a certain kind of “treasure” for which he travels to Egypt.',\n",
              " 'The key message is, “when you want something, all the universe conspires in helping you to achieve it.” Towards the final arc, Santiago gets robbed by bandits who end up revealing that the “treasure” he was looking for is buried in the place where his journey began.',\n",
              " 'The end.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naXZY2up-hxi"
      },
      "source": [
        "sentences_clean=[re.sub(r'[^\\w\\s]','',sentence.lower()) for sentence in sentence_tokens]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyr4HVfH-wRQ",
        "outputId": "5dbfbf9a-a44c-47d4-ac63-62deeccffd62"
      },
      "source": [
        "sentences_clean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['santiago is a shepherd who has a recurring dream which is supposedly prophetic',\n",
              " 'inspired on learning this he undertakes a journey to egypt to discover the meaning of life and fulfill his destiny',\n",
              " 'during the course of his travels he learns of his true purpose and meets many characters including an alchemist that teach him valuable lessons about achieving his dreams',\n",
              " 'santiago sets his sights on obtaining a certain kind of treasure for which he travels to egypt',\n",
              " 'the key message is when you want something all the universe conspires in helping you to achieve it towards the final arc santiago gets robbed by bandits who end up revealing that the treasure he was looking for is buried in the place where his journey began',\n",
              " 'the end']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gewl8ll-whJ"
      },
      "source": [
        "stop_words = stopwords.words('english')\n",
        "sentence_tokenss=[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6itqVa6X-31u"
      },
      "source": [
        "sentence_tokenss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcmZyJNe-6Yy"
      },
      "source": [
        "w2v=Word2Vec(sentence_tokenss,size=1,min_count=1,iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P8WWY_2_FxO",
        "outputId": "5a1d1588-b47c-467a-8d44-632ca2b5d2e5"
      },
      "source": [
        "sentence_Embedding = [[w2v[word][0] for word in words] for words in sentence_tokenss]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsxV1H-I_ete"
      },
      "source": [
        "sentence_tokenss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtRcgX8-_a2N"
      },
      "source": [
        "sentence_Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLOq9aPt_kfD"
      },
      "source": [
        "max_len=max([len(tokens) for tokens in sentence_tokenss])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgk3SSEc_3wb",
        "outputId": "00b2798f-6d68-48bd-bd9f-f72ff34cadbe"
      },
      "source": [
        "max_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVNdJDXz_muK"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJVce6W_yy5"
      },
      "source": [
        "sentence_embeddings=[np.pad(embedding,(0,max_len-len(embedding)),'constant') for embedding in sentence_Embedding] #padding "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0weV-INYAHQy"
      },
      "source": [
        "similarity_matrix = np.zeros([len(sentence_tokenss), len(sentence_tokenss)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj4irj_EAJia"
      },
      "source": [
        "for i,row_embedding in enumerate(sentence_embeddings):\n",
        "    for j,column_embedding in enumerate(sentence_embeddings):\n",
        "        similarity_matrix[i][j]=1-spatial.distance.cosine(row_embedding,column_embedding)  # calculating the cosine similairity between 1D array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDPaIRXuAtlY"
      },
      "source": [
        "nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz4FnE_4Az_p"
      },
      "source": [
        "top_sentence={sentence:scores[index] for index,sentence in enumerate(sentence_tokens)}\n",
        "top=dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b7UuyL7BOac"
      },
      "source": [
        "top"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAEZNYiyCzwM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOIIyL-nCzyn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r38TG66ZCz2H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvR5bv8ICz4O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl74a0sQCz7D"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/NLP/unsupervised Text summary/tennis_articles.csv\", encoding= 'unicode_escape')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "pfTe-ARKDPZQ",
        "outputId": "313fa385-02d7-4c54-ca29-f1640f976dc0"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>article_title</th>\n",
              "      <th>article_text</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I do not have friends in tennis, says Maria Sh...</td>\n",
              "      <td>Maria Sharapova has basically no friends as te...</td>\n",
              "      <td>https://www.tennisworldusa.org/tennis/news/Mar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Federer defeats Medvedev to advance to 14th Sw...</td>\n",
              "      <td>BASEL, Switzerland (AP)  Roger Federer advanc...</td>\n",
              "      <td>http://www.tennis.com/pro-game/2018/10/copil-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Tennis: Roger Federer ignored deadline set by ...</td>\n",
              "      <td>Roger Federer has revealed that organisers of ...</td>\n",
              "      <td>https://scroll.in/field/899938/tennis-roger-fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Nishikori to face off against Anderson in Vien...</td>\n",
              "      <td>Kei Nishikori will try to end his long losing ...</td>\n",
              "      <td>http://www.tennis.com/pro-game/2018/10/nishiko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Roger Federer has made this huge change to ten...</td>\n",
              "      <td>Federer, 37, first broke through on tour over ...</td>\n",
              "      <td>https://www.express.co.uk/sport/tennis/1036101...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>Rafael Nadal: World No 1 ARRIVES for Paris Mas...</td>\n",
              "      <td>Nadal has not played tennis since he was force...</td>\n",
              "      <td>https://www.express.co.uk/sport/tennis/1037119...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>TENNIS.COM PODCAST: POINT DEFENSE, RANKING DRO...</td>\n",
              "      <td>Tennis giveth, and tennis taketh away. The end...</td>\n",
              "      <td>http://www.tennis.com/pro-game/2018/10/tennisc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>Tennis journalists heartbreaking insight on T...</td>\n",
              "      <td>I PLAYED golf last week with Todd Reid. He pic...</td>\n",
              "      <td>https://www.foxsports.com.au/tennis/tennis-jou...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article_id  ...                                             source\n",
              "0           1  ...  https://www.tennisworldusa.org/tennis/news/Mar...\n",
              "1           2  ...  http://www.tennis.com/pro-game/2018/10/copil-s...\n",
              "2           3  ...  https://scroll.in/field/899938/tennis-roger-fe...\n",
              "3           4  ...  http://www.tennis.com/pro-game/2018/10/nishiko...\n",
              "4           5  ...  https://www.express.co.uk/sport/tennis/1036101...\n",
              "5           6  ...  https://www.express.co.uk/sport/tennis/1037119...\n",
              "6           7  ...  http://www.tennis.com/pro-game/2018/10/tennisc...\n",
              "7           8  ...  https://www.foxsports.com.au/tennis/tennis-jou...\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZtyE20PC0lG"
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def text_cleaner(text):\n",
        "  newString  = text.lower()\n",
        "  newString = re.sub(r'\\([^)]*\\)', '', newString)  # to remove paranthesys\n",
        "  newString = re.sub('\"','', newString) # to remove 's \n",
        "  newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])  # change the contracting words\n",
        "  newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "  newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "  tokens = [w for w in newString.split() if not w in stop_words]  #Remove stopwords\n",
        "  return tokens\n",
        "\n",
        "sentence_tokens = []\n",
        "\n",
        "for t in data.article_text:\n",
        "  sentence_tokens.append(sent_tokenize(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEe3N0dQFbxL"
      },
      "source": [
        "sentence_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXKGakXvFbzU"
      },
      "source": [
        "sentence = [y for x in sentence_tokens for y in x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KckpB89HFj13"
      },
      "source": [
        "cleaned_texts = []\n",
        "for t in sentence:\n",
        "  cleaned_texts.append(text_cleaner(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHoImIXcFtxF"
      },
      "source": [
        "cleaned_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKBEOXKqDffX"
      },
      "source": [
        "w2v = Word2Vec(cleaned_texts, size =1, min_count=1, iter = 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiG9snKHDxZV",
        "outputId": "57037363-380f-4baa-e13f-ea3d0319d307"
      },
      "source": [
        "sentence_Embedding = [[w2v[word][0] for word in words] for words in cleaned_texts]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-88I69uEEGza"
      },
      "source": [
        "max_len = max([len(word) for word in cleaned_texts])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlzxwl69EBgh"
      },
      "source": [
        "sentence_embeddings=[np.pad(embedding,(0,max_len-len(embedding)),'constant') for embedding in sentence_Embedding] #padding "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgp_7w3JEX8J"
      },
      "source": [
        "similarity_matrix = np.zeros([len(cleaned_texts), len(cleaned_texts)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rD4rocYEiyg",
        "outputId": "ff1886d2-eaa0-4af4-aafb-38c597378797"
      },
      "source": [
        "for i,row_embedding in enumerate(sentence_embeddings):\n",
        "    for j,column_embedding in enumerate(sentence_embeddings):\n",
        "        similarity_matrix[i][j]=1-spatial.distance.cosine(row_embedding,column_embedding)  # calculating the cosine similairity between 1D array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKNKMtWmEqNB"
      },
      "source": [
        "nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "scores = nx.pagerank(nx_graph, max_iter=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H9Ahz-LEryS"
      },
      "source": [
        "top_sentence={sentencee:scores[index] for index,sentencee in enumerate(sentence)}\n",
        "top=dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:4])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}